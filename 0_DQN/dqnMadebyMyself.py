import numpy as np
import torch
import torch.optim as optim
import torch.nn.functional as F
import random
from ExperienceReplayBuffer import ReplayBuffer
from neuralNetwork import DQN_Network

class DQNAgent:
    def __init__(self, state_dim, action_dim, epsilon_steps=20000, learning_rate=0.001, gamma=0.99):
        #åˆå§‹åŒ–DQNå‚æ•°
        self.state_dim = state_dim  #çŠ¶æ€ç©ºé—´ç»´æ•°
        self.action_dim = action_dim    #åŠ¨ä½œç©ºé—´ç»´æ•°
        self.main_buffer = ReplayBuffer(500000)
        self.success_buffer = ReplayBuffer(100000)
        self.batch_size = 64    #é‡‡æ ·å¤§å°
        self.success_batch_percent = 0.8    #æˆåŠŸç¼“å†²åŒºé‡‡æ ·æ¯”ä¾‹
        self.gamma = gamma
        self.learning_rate = learning_rate
        self.epsilon_max = 1
        self.epsilon = self.epsilon_max
        self.epsilon_min = 0.005
        self.epsilon_decay = epsilon_steps    #Îµçº¿æ€§è¡°å‡ï¼Œæ‰åˆ°Îµ_minçš„episode
        self.steps = 0

        # è®¾ç½®é‡å¤kæ¬¡æ¢ç´¢
        self.repeat_action = None
        self.repeat_action_number = 0

        #åˆå§‹åŒ–ä¸¤ä¸ªç½‘ç»œ
        self.main_network = DQN_Network(state_dim, action_dim)
        self.target_network = DQN_Network(state_dim, action_dim)
        self.optimizer = optim.Adam(self.main_network.parameters(), lr=learning_rate)

        #åŒæ­¥ç›®æ ‡ç½‘ç»œå‚æ•°
        self.update_target_network()    #åˆå§‹åŒ–ç±»å°±ç›´æ¥æ‰§è¡Œ
    
    def update_epsilon(self):
        if self.epsilon >=  self.epsilon_min :
            self.epsilon -= ( self.epsilon_max - self.epsilon_min ) / ( self.epsilon_decay )

    def update_target_network(self):
        self.target_network.load_state_dict(self.main_network.state_dict())
    
    # æ·»åŠ ç»éªŒ
    def add_experience(self, buffer, state, action, reward, next_state, done):
        # print(f"state={state},action={action},reward={reward},next_state={next_state},done={done}")
        if buffer == 'main_buffer':
            self.main_buffer.add_experience(state, action, reward, next_state, done)
        elif buffer == 'success_buffer':
            self.success_buffer.add_experience(state, action, reward, next_state, done)

    # #ä½¿ç”¨Îµ-greedyé€‰æ‹©åŠ¨ä½œ
    # def choose_action(self,state):
    #     # é€‰æ‹©æ¢ç´¢ï¼Œéšä¾¿é€‰åŠ¨ä½œç©ºé—´é‡Œçš„ä¸€ä¸ªåŠ¨ä½œ
    #     if np.random.rand() <= self.epsilon_max:
    #         return random.randrange(self.action_dim)
    #     #é€‰æ‹©åˆ©ç”¨ï¼Œé€‰q hatæœ€å¤§çš„
    #     #unsqueezeè§£è¯»ï¼šunsqueezeå°±æ˜¯å°†å…¶å‡ç»´å˜æˆäºŒç»´çŸ©é˜µï¼Œ0è¡¨ç¤ºåœ¨ä¹‹å‰å¡«å……ç»´åº¦ï¼Œ1è¡¨ç¤ºåœ¨ä¹‹åå¡«å……ç»´åº¦
    #     #æ¯”å¦‚(3)ğŸ‘ˆ3ç»´è¡Œå‘é‡ï¼Œunsqueeze(0)å˜æˆ(1,3)ï¼Œ1è¡Œ3åˆ—çŸ©é˜µï¼Œåœ¨3ä¹‹å‰çš„ç»´åº¦æ’å…¥ä¸€ä¸ªç»´åº¦ã€‚
    #     state_tensor = torch.FloatTensor(state).unsqueeze(0)
    #     q_values = self.target_network(state_tensor)  #è®¡ç®—q value
    #     return torch.argmax(q_values).item()    #è¿”å›æœ€å¤§çš„qæ‰€å¯¹åº”çš„action
    
    #ä½¿ç”¨Îµ-greedyé€‰æ‹©åŠ¨ä½œ
    def choose_action(self,state):
        # é€‰æ‹©æ¢ç´¢
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_dim),np.random.randint(5,13)    #éšä¾¿é€‰åŠ¨ä½œç©ºé—´é‡Œçš„åŠ¨ä½œ
        #é€‰æ‹©åˆ©ç”¨ï¼Œé€‰q hatæœ€å¤§çš„
        #unsqueezeè§£è¯»ï¼šunsqueezeå°±æ˜¯å°†å…¶å‡ç»´å˜æˆäºŒç»´çŸ©é˜µï¼Œ0è¡¨ç¤ºåœ¨ä¹‹å‰å¡«å……ç»´åº¦ï¼Œ1è¡¨ç¤ºåœ¨ä¹‹åå¡«å……ç»´åº¦
        #æ¯”å¦‚(3)ğŸ‘ˆ3ç»´è¡Œå‘é‡ï¼Œunsqueeze(0)å˜æˆ(1,3)ï¼Œ1è¡Œ3åˆ—çŸ©é˜µï¼Œåœ¨3ä¹‹å‰çš„ç»´åº¦æ’å…¥ä¸€ä¸ªç»´åº¦ã€‚
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        # ä¼ ç»ŸDQN
        q_values = self.main_network(state_tensor)  #è®¡ç®—q value
        return torch.argmax(q_values).item(),1    #è¿”å›æœ€å¤§çš„qæ‰€å¯¹åº”çš„actionå’Œæ‰§è¡Œæ¬¡æ•°
        # Double DQNï¼šç”¨
        #
    
    #é‡‡æ ·æ•°æ®å¹¶è®­ç»ƒå‡½æ•°
    def train(self):
        if self.main_buffer.size < self.batch_size * 10:
            return  #æ²¡æœ‰ç»éªŒï¼Œæ²¡æ³•è®­ç»ƒ
        #æœ‰ç»éªŒï¼Œè®­ç»ƒ
        #é‡‡æ ·ï¼Œæ ¹æ®success_batch_persenté€‰æ‹©
        if self.success_buffer.size < self.batch_size * self.success_batch_percent :
            state, actions, rewards, next_state, done = self.main_buffer.sample(self.batch_size)
        else :
            state, actions, rewards, next_state, done = self.main_buffer.sample(int(self.batch_size * (1 - self.success_batch_percent)))
            state_success, actions_success, rewards_success, next_state_success, done_success = self.success_buffer.sample(int(self.batch_size * self.success_batch_percent))
            state = np.concatenate([state, state_success])
            actions = np.concatenate([actions, actions_success])
            rewards = np.concatenate([rewards, rewards_success])
            next_state = np.concatenate([next_state, next_state_success])
            done = np.concatenate([done, done_success])
        
        state = torch.FloatTensor(state)
        actions = torch.IntTensor(actions)
        rewards = torch.FloatTensor(rewards)
        dones = torch.BoolTensor(done)
        next_state = torch.FloatTensor(next_state)

        #è®¡ç®—å½“å‰qå€¼
        # è¿™å¥ä»£ç çš„æ„æ€æ˜¯ï¼Œæˆ‘ä»¬å°†stateä¹Ÿå°±æ˜¯positions,speedè¿™ä¸ªäºŒç»´çŸ©é˜µä¼ å…¥main_networkè¿›è¡Œå‰å‘ä¼ æ’­
        # æ¯”å¦‚æˆ‘ä»¬çš„stateæ¯è¡Œè¡¨ç¤ºposition1,speed1,æ¯æ¬¡ç¥ç»ç½‘ç»œè¾“å…¥è¯¥è¡Œï¼Œç„¶åè¾“å‡ºä¸€ä¸ªè¡Œå‘é‡ï¼Œå¾—åˆ°q hat=(qhat_action1,qhat_action2,...)
        # å°†ç»“æœqhatä½¿ç”¨gatherè·å–actionsçš„å¯¹åº”q hatï¼Œå¹¶å°†è¿™ä¸ª1åˆ—nè¡Œçš„äºŒç»´å¼ é‡èµ‹å€¼ç»™current_q_values
        # å…¶ä¸­gather(dim=1)è¡¨ç¤ºåœ¨qhatçš„æ¯è¡ŒæŒ‰åˆ—è¿›è¡Œç­›é€‰ã€‚
        #ï¼ˆqhatæ˜¯äºŒç»´çŸ©é˜µï¼Œæ¯è¡Œæ˜¯æŸä¸ªstateçš„æ‰€æœ‰rewardï¼Œç”¨ä¸‹æ ‡æ¥è¡¨ç¤ºè¿™æ˜¯ç¬¬å‡ ä¸ªactionå¯¹åº”çš„rewardï¼‰
        #ï¼ˆactionsæ˜¯è¡Œå‘é‡ä½¿ç”¨unsqeeze(1)å°†å…¶å˜ä¸ºäºŒç»´çš„nè¡Œ1åˆ—çŸ©é˜µï¼‰
        current_q_values = self.main_network(state).gather(1, actions.unsqueeze(1)).squeeze()

        #è®¡ç®—ç›®æ ‡Qå€¼
        with torch.no_grad():
            # maxè¡¨ç¤ºåœ¨1ç»´åº¦ä¸Šå–æœ€å¤§å€¼ï¼Œè¿”å›(value,index)ï¼Œ[0]è¡¨ç¤ºåªè¦æœ€å¤§å€¼ä¸è¦ç´¢å¼•
            next_q_values = self.target_network(next_state).max(1)[0]
            # ä½¿ç”¨è´å°”æ›¼æœ€ä¼˜å…¬å¼è®¡ç®—TD targetï¼Œå³ç”¨ä¸‹ä¸€æ—¶åˆ»çš„q hatæœ€å¤§å€¼æ¥ä½œä¸ºqçš„ä¼°è®¡ã€‚
            # ç”±äºä½¿ç”¨äº†ç»éªŒå›æ”¾ï¼Œè¿™é‡Œçš„next_stateéƒ½æ˜¯å·²ç»çŸ¥é“çš„ã€‚
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)
            # print(f"next_q_values={next_q_values}")
        
        # ä½¿ç”¨MSEè®¡ç®—æŸå¤±ï¼ŒMSElossåªèƒ½æ¥å—å‘é‡ï¼Œä¸èƒ½æ¥å—äºŒç»´çš„çŸ©é˜µ
        loss = F.mse_loss(current_q_values, target_q_values)

        #åå‘ä¼ æ’­å’Œä¼˜åŒ–
        self.optimizer.zero_grad()
        loss.backward()
        # torch.nn.utils.clip_grad_norm_(self.main_network.parameters(), max_norm=10)
        self.optimizer.step()

        #æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.steps += 1
        if self.steps % 50 == 0:
            self.update_target_network()
        
        #æ›´æ–°epsilon
        self.update_epsilon()

        # æ‰“å°ä¸€äº›è°ƒè¯•å‚æ•°
        # if self.steps % 1500 == 0:
        #     print(f"Step {self.steps}:")
        #     print(f"  Loss: {loss.item():.4f}")
        #     print(f"  Epsilon: {self.epsilon:.3f}")
        #     print(f"  Avg Q: {current_q_values.mean().item():.3f}")
        #     print(f"  Buffer size: {self.main_buffer.size}")
        
    def save_model(self, file_path):
        # pass
        torch.save(self.main_network.state_dict(), file_path)
    
    def load_model(self, file_path):
        # pass
        self.main_network.load_state_dict(torch.load(file_path))
        self.update_target_network()  # åŒæ­¥åˆ°ç›®æ ‡ç½‘ç»œ
